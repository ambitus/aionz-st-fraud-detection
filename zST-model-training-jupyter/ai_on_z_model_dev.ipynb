{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5806b2c1",
   "metadata": {},
   "source": [
    "# AI on IBM Z Model Development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdb345d",
   "metadata": {},
   "source": [
    "## Import required python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488fdfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "\n",
    "# Model training\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "# Data preprocessing\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler\n",
    "\n",
    "# PMML\n",
    "from sklearn2pmml import sklearn2pmml\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn2pmml.pipeline import PMMLPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8fd50d",
   "metadata": {},
   "source": [
    "## Input dataset and label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620c4391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User must provide filepath to dataset and label name\n",
    "DATASET_FILENAME = 'datasets/credit_card_transactions-ibm_v2.csv'\n",
    "DATASET_LABEL_NAME = 'Is Fraud?'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62e3ecf",
   "metadata": {},
   "source": [
    "## Split features and labels from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982a8db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_features_and_lables(dataset_df, label):\n",
    "    features = dataset_df.copy()\n",
    "    labels = features.pop(label)\n",
    "\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b219100b",
   "metadata": {},
   "source": [
    "## Machine learning model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c551e866",
   "metadata": {},
   "source": [
    "### Train any model using cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724eb81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_pipeline(model_type, name, hyperparameters, X, y, cat_feats, num_feats):\n",
    "    print('Splitting dataset for training and testing...\\n')\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "    print('Training ' + name + '...\\n')\n",
    "    start_time = time.time()\n",
    "    \n",
    "    feature_transformers = []\n",
    "    if len(num_feats) > 0:\n",
    "        numeric_transformer = Pipeline(steps=[('normalizer', StandardScaler())])\n",
    "        feature_transformers.append(('num', numeric_transformer, num_feats))\n",
    "    \n",
    "    if len(cat_feats) > 0:\n",
    "        categorical_transformer = Pipeline(steps=[('encoder', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))])\n",
    "        feature_transformers.append(('cat', categorical_transformer, cat_feats))\n",
    "    \n",
    "    preprocessor = ColumnTransformer(transformers=feature_transformers)\n",
    "\n",
    "    ai_pipeline = PMMLPipeline([\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"classifier\", model_type)\n",
    "    ])\n",
    "\n",
    "    # train using cross validation\n",
    "    clf = GridSearchCV(ai_pipeline, hyperparameters)\n",
    "    clf.fit(X=X_train, y=y_train)\n",
    "    \n",
    "    print('\\tOptimal hyperparameters:')\n",
    "    print('\\t' + str(clf.best_params_))\n",
    "\n",
    "    # save best performing model\n",
    "    best_pipeline = clf.best_estimator_\n",
    "\n",
    "    print('\\tFeature importances:')\n",
    "    for name, importance in zip(X_train.columns, best_pipeline.named_steps.classifier.feature_importances_):\n",
    "        print(name, \"=\", importance)\n",
    "\n",
    "    end_time = time.time()\n",
    "    total_time = (end_time - start_time) / 60\n",
    "\n",
    "    print('\\n\\tTraining time (mins): \\n\\t' + str(total_time) + '\\n')\n",
    "\n",
    "    # evaluate model performance\n",
    "    y_pred = best_pipeline.predict(X_test)\n",
    "\n",
    "    if classification_problem:\n",
    "        # this will work successfully for a classification problem\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred, average='weighted')\n",
    "        recall = recall_score(y_test, y_pred, average='weighted')\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    else:\n",
    "        # this is a regression problem\n",
    "        # above scoring functions don't work for regression\n",
    "        accuracy = clf.score(X_test, y_test)\n",
    "        precision = 'N/A'\n",
    "        recall = 'N/A'\n",
    "        f1 = 'N/A'\n",
    "\n",
    "    # Export pipeline in pmml format\n",
    "    sklearn2pmml(best_pipeline, 'models/' + name + \".xml\", with_repr=True)\n",
    "\n",
    "    return best_pipeline, precision, recall, f1, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241bc6eb",
   "metadata": {},
   "source": [
    "### Scikit-learn model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd065c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sckit-learn - Random Forest (Classification)\n",
    "def train_random_forest(X, y, cat_feats, num_feats):\n",
    "\n",
    "    # hyperparameters\n",
    "    n_estimators = [10] #[5, 10, 100, 500]\n",
    "    criterions = ['entropy'] #['gini', 'entropy']\n",
    "    max_features = [None] #[None , 'auto', 'sqrt', 'log2']\n",
    "    max_depths = [None] #[None , 1000, 100, 10, 5]\n",
    "    min_samples_splits = [2] #[2 , 10, 100]\n",
    "    min_samples_leafs = [1] #[1 , 5, 10, 50]\n",
    "    max_leaf_nodes = [None] #[None , 5, 10, 100]\n",
    "    bootstraps = [True] #[True, False]\n",
    "    random_states = [33]\n",
    "\n",
    "    hyperparameters = {'classifier__n_estimators': n_estimators,\n",
    "                       'classifier__random_state': random_states,\n",
    "                       'classifier__criterion': criterions,\n",
    "                       'classifier__max_features': max_features,\n",
    "                       'classifier__max_depth': max_depths,\n",
    "                      'classifier__min_samples_split': min_samples_splits,\n",
    "                      'classifier__min_samples_leaf': min_samples_leafs,\n",
    "                      'classifier__max_leaf_nodes': max_leaf_nodes,\n",
    "                      'classifier__bootstrap': bootstraps\n",
    "                     }\n",
    "\n",
    "    return training_pipeline(RandomForestClassifier(), 'RandomForestClassifier', hyperparameters, X, y, cat_feats, num_feats)\n",
    "\n",
    "\n",
    "# Scikit-learn - Gradient Boosting Tree (Classification)\n",
    "def train_gradient_boosted_tree_sklearn(X, y, cat_feats, num_feats):\n",
    "\n",
    "    # hyperparameters\n",
    "    # base_estimators = [base_estimator] #[None, base_estimator]\n",
    "    n_estimators = [10] #[5, 10, 50, 100, 500]\n",
    "    learning_rates = [0.9] #[1.0, 0.9, 0.1, 0.0001]\n",
    "    # loss = ['exponential'] #['log_loss', 'exponential']\n",
    "    random_states = [33]\n",
    "\n",
    "    hyperparameters = {'classifier__n_estimators': n_estimators,\n",
    "                       'classifier__learning_rate': learning_rates,\n",
    "                    #    'classifier__loss': loss,\n",
    "                       'classifier__random_state': random_states\n",
    "                      }\n",
    "\n",
    "    return training_pipeline(GradientBoostingClassifier(), 'GradientBoostingClassifier', hyperparameters, X, y, cat_feats, num_feats)\n",
    "\n",
    "\n",
    "# Sckit-learn - Random Forest (Regression)\n",
    "def train_random_forest_regression(X, y, cat_feats, num_feats):\n",
    "\n",
    "    # hyperparameters\n",
    "    n_estimators = [10] #[5, 10, 100, 500]\n",
    "    max_features = [None] #[None , 'auto', 'sqrt', 'log2']\n",
    "    max_depths = [None] #[None , 1000, 100, 10, 5]\n",
    "    min_samples_splits = [2] #[2 , 10, 100]\n",
    "    min_samples_leafs = [1] #[1 , 5, 10, 50]\n",
    "    max_leaf_nodes = [None] #[None , 5, 10, 100]\n",
    "    bootstraps = [True] #[True, False]\n",
    "    random_states = [33]\n",
    "\n",
    "    hyperparameters = {'classifier__n_estimators': n_estimators,\n",
    "                       'classifier__random_state': random_states,\n",
    "                       'classifier__max_features': max_features,\n",
    "                       'classifier__max_depth': max_depths,\n",
    "                      'classifier__min_samples_split': min_samples_splits,\n",
    "                      'classifier__min_samples_leaf': min_samples_leafs,\n",
    "                      'classifier__max_leaf_nodes': max_leaf_nodes,\n",
    "                      'classifier__bootstrap': bootstraps\n",
    "                     }\n",
    "\n",
    "    return training_pipeline(RandomForestRegressor(), 'RandomForestRegressor', hyperparameters, X, y, cat_feats, num_feats)\n",
    "\n",
    "# Scikit-learn - Gradient Boosting Tree (Regression)\n",
    "def train_gradient_boosted_tree_sklearn_regression(X, y, cat_feats, num_feats):\n",
    "\n",
    "    # hyperparameters\n",
    "    # base_estimators = [base_estimator] #[None, base_estimator]\n",
    "    n_estimators = [10] #[5, 10, 50, 100, 500]\n",
    "    learning_rates = [0.9] #[1.0, 0.9, 0.1, 0.0001]\n",
    "    # loss = ['exponential'] #['log_loss', 'exponential']\n",
    "    random_states = [33]\n",
    "\n",
    "    hyperparameters = {'classifier__n_estimators': n_estimators,\n",
    "                       'classifier__learning_rate': learning_rates,\n",
    "                    #    'classifier__loss': loss,\n",
    "                       'classifier__random_state': random_states\n",
    "                      }\n",
    "\n",
    "    return training_pipeline(GradientBoostingRegressor(), 'GradientBoostingRegressor', hyperparameters, X, y, cat_feats, num_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426b890e",
   "metadata": {},
   "source": [
    "### Train models with different frameworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3525712a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper function to train any model type\n",
    "def train_model(model_type, X, y, cat_feats, num_feats):\n",
    "    pipeline = None\n",
    "    precision = None\n",
    "    recall = None\n",
    "    f1 = None\n",
    "    num_features = X.shape[1]\n",
    "\n",
    "    if model_type == 'random_forest_sklearn':\n",
    "        pipeline, precision, recall, f1, accuracy = train_random_forest(X, y, cat_feats, num_feats)\n",
    "    elif model_type == 'gradient_boosted_tree_sklearn':\n",
    "        pipeline, precision, recall, f1, accuracy = train_gradient_boosted_tree_sklearn(X, y, cat_feats, num_feats)\n",
    "    elif model_type == 'random_forest_sklearn_regression':\n",
    "        pipeline, precision, recall, f1, accuracy = train_random_forest_regression(X, y, cat_feats, num_feats)\n",
    "    elif model_type == 'gradient_boosted_tree_sklearn_regression':\n",
    "        pipeline, precision, recall, f1, accuracy = train_gradient_boosted_tree_sklearn_regression(X, y, cat_feats, num_feats)\n",
    "        \n",
    "    return pipeline, precision, recall, f1, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d283b0d9",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb3d6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaning(dataset_df):\n",
    "    print('Performing basic data cleaning...')\n",
    "    \n",
    "    # Data cleaning - remove unneeded chars\n",
    "    print('Removing unneeded special characters...')\n",
    "    cols = dataset_df.columns\n",
    "    dataset_df[cols] = dataset_df[cols].replace({'\\$': '', ',': '', ':': '', '\\+': '', '\\+': '', '\\#': '', '\\/': ''}, regex=True)\n",
    "\n",
    "    # Check for missing values\n",
    "    print('Number of missing values:\\n' + str(dataset_df.isna().sum()) + '\\n')\n",
    "\n",
    "    print('Replacing missing values and 2nd pass for num/cat determination...')\n",
    "\n",
    "    categorical_features = list(dataset_df.select_dtypes(include = \"object\").columns)\n",
    "    \n",
    "    cat_feats = []\n",
    "    num_feats = []\n",
    "\n",
    "    num_feats = list(dataset_df.select_dtypes(exclude='object').columns)\n",
    "    dataset_df[num_feats] = dataset_df[num_feats].fillna(0)\n",
    "\n",
    "    \n",
    "    for feature in categorical_features:\n",
    "        feature_value = dataset_df.loc[1, feature]\n",
    "\n",
    "        try:\n",
    "            if str(feature_value).replace('.','').replace('-','').isnumeric() == False:\n",
    "                print(feature +' type is STRING')\n",
    "                # Replace any NaN's with 'na'\n",
    "                dataset_df[feature] = dataset_df[feature].fillna('na')\n",
    "                cat_feats.append(feature)\n",
    "            elif str(feature_value).replace('.','').replace('-','').isnumeric() == True:\n",
    "                print(feature +' type is NUMERIC')\n",
    "                dataset_df[feature] = dataset_df[feature].astype(np.float64) if '.' in str(feature_value) else dataset_df[feature].astype(np.int64) \n",
    "                num_feats.append(feature)\n",
    "            else:\n",
    "                print(feature +' type is UNKNOWN')\n",
    "                dataset_df[feature] = dataset_df[feature].fillna('na')\n",
    "                cat_feats.append(feature)                    \n",
    "        except:\n",
    "            print(feature +' gives an ERROR')\n",
    "            dataset_df[feature] = dataset_df[feature].fillna('na')\n",
    "            cat_feats.append(feature)\n",
    "\n",
    "    dataset_df[cat_feats] = dataset_df[cat_feats].astype(str)\n",
    "    \n",
    "    # COBOL supports numeric literals up to 18 digits in length. Below section truncates the numeric \n",
    "    # features if they exceed 18 digits. So that the same data is used for training, testing and inferencing.\n",
    "    # Alternatively these columns can be reclasified from numerical to categorical features\n",
    "    cobol_max_number = (10 ** 18) - 1 # max number of 9's\n",
    "    max_values = dataset_df[num_feats].max(numeric_only=True)\n",
    "    feat_index = 0\n",
    "    for max_value in max_values:\n",
    "        if abs(max_value) > cobol_max_number:\n",
    "            # Downscale factor in powers of 10. COBOL takes significant 18 digits and truncates remaining\n",
    "            # e.g. for features as 'Merchant Name' which are 19 digits, the downscale_factor will be 10\n",
    "            downscale_factor = 10 ** round(math.log10(max_value/cobol_max_number))\n",
    "            dataset_df[num_feats[feat_index]] = (dataset_df[num_feats[feat_index]]/downscale_factor).astype(np.int64)\n",
    "        feat_index += 1\n",
    "\n",
    "    print('Data cleaning successful!\\n')\n",
    "\n",
    "    return dataset_df, cat_feats, num_feats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c073736",
   "metadata": {},
   "source": [
    "## Fetch and process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4d331f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Currently, datasets for the following AI on IBM Z Solution Templates are supported:\n",
    "# - Fraud Detection\n",
    "# - Credit Risk Assessment\n",
    "\n",
    "# Verify that required features are in dataset\n",
    "required_features = []\n",
    "required_features_CCF = ['User', 'Card', 'Year', 'Month', 'Day', 'Time', 'Amount', 'Use Chip', 'Merchant Name', 'Merchant City', 'Merchant State', 'Zip', 'MCC', 'Errors',  'Is Fraud?']\n",
    "required_features_CRA = ['person_age', 'person_income', 'person_home_ownership', 'person_emp_length', 'loan_intent', 'loan_grade', 'loan_amnt', 'loan_int_rate', 'loan_status', 'loan_percent_income', 'cb_person_default_on_file', 'cb_person_cred_hist_length']\n",
    "required_features_HIC = ['applicant_id', 'years_of_insurance_with_us', 'regular_checkup_last_year', 'adventure_sports', 'Occupation', 'visited_doctor_last_1_year', 'cholesterol_level', 'daily_avg_steps', 'age', 'heart_decs_history', 'Any_other_major_decs_history', 'Gender', 'avg_glucose_level', 'bmi', 'smoking_status', 'Year_last_admitted', 'Location', 'weight', 'covered_by_any_other_company', 'Alcohol', 'exercise', 'weight_change_in_last_one_year', 'fat_percentage', 'insurance_cost']\n",
    "\n",
    "dataset_df = pd.read_csv(DATASET_FILENAME, nrows=2)\n",
    "\n",
    "if set(required_features_CCF).issubset(dataset_df.columns):\n",
    "    required_features = required_features_CCF\n",
    "elif set(required_features_CRA).issubset(dataset_df.columns):\n",
    "    required_features = required_features_CRA\n",
    "elif set(required_features_HIC).issubset(dataset_df.columns):\n",
    "    required_features = required_features_HIC\n",
    "    \n",
    "# Load dataset from file\n",
    "if len(required_features) != 0:\n",
    "    print('Loading dataset...')\n",
    "    \n",
    "    # In order to minimize training time, we will only use a subset of the provided data (keep every nth line)\n",
    "    n = 10\n",
    "    dataset_df = pd.read_csv(DATASET_FILENAME, header=0, usecols=required_features, skiprows=lambda i: i % n != 0)\n",
    "    \n",
    "    # If you want to use the complete dataset, uncomment the follow line and comment the above code\n",
    "    # dataset_df = pd.read_csv(DATASET_FILENAME, usecols=required_features)\n",
    "    \n",
    "    print('Dataset shape: ' + str(dataset_df.shape) + '\\n')\n",
    "\n",
    "    X, y = split_features_and_lables(dataset_df, DATASET_LABEL_NAME)\n",
    "\n",
    "    X, cat_feats, num_feats = data_cleaning(X)\n",
    "else:\n",
    "    print('Please provide dataset with the following features:')\n",
    "    print(required_features)\n",
    "    dataset_df = None\n",
    "    X = None\n",
    "    y = None\n",
    "    cat_feats = None\n",
    "    num_feats = None\n",
    "\n",
    "print('AI Dataset (cleaned):')\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01390bf",
   "metadata": {},
   "source": [
    "## Train machine learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970edb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_details = []\n",
    "\n",
    "\n",
    "# Determine if this is a classification or regression problem\n",
    "classification_problem = True\n",
    "\n",
    "if len(y.unique()) > 10:\n",
    "    classification_problem = False\n",
    "\n",
    "if classification_problem:\n",
    "    model_rf_sklearn, precision, recall, f1, accuracy = train_model('random_forest_sklearn', X, y, cat_feats, num_feats)\n",
    "    model_details_rf_sklearn = {\n",
    "        'name': 'random_forest_sklearn',\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1\n",
    "    }\n",
    "    model_details.append(model_details_rf_sklearn)\n",
    "\n",
    "    model_gb_sklearn, precision, recall, f1, accuracy = train_model('gradient_boosted_tree_sklearn', X, y, cat_feats, num_feats)\n",
    "    model_details_gb_sklearn = {\n",
    "        'name': 'gradient_boosted_tree_sklearn',\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1\n",
    "    }\n",
    "    model_details.append(model_details_gb_sklearn)\n",
    "\n",
    "else:\n",
    "    print('\\t** this is a regression problem, using regression models instead **\\n')\n",
    "    \n",
    "    model_rf_sklearn_rg, precision, recall, f1, accuracy = train_model('random_forest_sklearn_regression', X, y, cat_feats, num_feats)\n",
    "    model_details_rf_sklearn_rg = {\n",
    "        'name': 'random_forest_sklearn_regression',\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1\n",
    "    }\n",
    "    model_details.append(model_details_rf_sklearn_rg)\n",
    "    \n",
    "    model_gb_sklearn_rg, precision, recall, f1, accuracy = train_model('gradient_boosted_tree_sklearn_regression', X, y, cat_feats, num_feats)\n",
    "    model_details_gb_sklearn_rg = {\n",
    "        'name': 'gradient_boosted_tree_sklearn_regression',\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1\n",
    "    }\n",
    "    model_details.append(model_details_gb_sklearn_rg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2617e10b",
   "metadata": {},
   "source": [
    "## Recommend best ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33342f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_details_sorted = sorted(model_details, key=lambda d: d['accuracy'], reverse=True)\n",
    "print('AI model details (ranked by best performance)')\n",
    "print(json.dumps(model_details_sorted, indent=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
